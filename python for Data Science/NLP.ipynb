{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5f5b09-b456-48b5-ac44-73bcd3788105",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP):\n",
    "\n",
    "###### **NLTK**: Natural Language Toolkit for various NLP tasks.\n",
    "###### **spaCy**: Industrial-strength NLP library focusing on efficiency.\n",
    "###### **gensim**: Topic modeling and document similarity library.\n",
    "###### **Transformers (Hugging Face)**: Pre-trained NLP models and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b347b77-60e3-4c6a-b012-922cea8e02ca",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb28c1-41cc-4d7b-af72-37b447fe6bb4",
   "metadata": {},
   "source": [
    "import nltk  \n",
    "from nltk.corpus import stopwords, wordnet  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.probability import FreqDist  \n",
    "from nltk.tag import pos_tag  \n",
    "from nltk.chunk import ne_chunk  \n",
    "from nltk.translate import bleu_score  \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  \n",
    "from nltk.parse import CoreNLPParser  \n",
    "from nltk.translate.meteor_score import single_meteor_score  \n",
    "\n",
    "#### 1. Downloading NLTK Resources (One-time)\n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('averaged_perceptron_tagger')  \n",
    "nltk.download('wordnet')  \n",
    "nltk.download('maxent_ne_chunker')  \n",
    "nltk.download('words')  \n",
    "nltk.download('vader_lexicon')  \n",
    "\n",
    "#### 2. Tokenization\n",
    "text = \"NLTK is a powerful library for natural language processing.\"  \n",
    "tokens = word_tokenize(text)  \n",
    "\n",
    "#### 3. Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  \n",
    "\n",
    "#### 4. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in filtered_tokens]  \n",
    "\n",
    "#### 5. Sentence Tokenization\n",
    "sentences = sent_tokenize(text)  \n",
    "\n",
    "#### 6. Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(tokens)  \n",
    "\n",
    "#### 7. Named Entity Recognition\n",
    "ne_tree = ne_chunk(pos_tags)  \n",
    "\n",
    "#### 8. Frequency Distribution\n",
    "fdist = FreqDist(tokens)  \n",
    "\n",
    "#### 9. Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()  \n",
    "sentiment_score = sia.polarity_scores(text)  \n",
    "\n",
    "#### 10. Machine Translation Evaluation (BLEU and METEOR)\n",
    "reference = [\"NLTK is a powerful library for natural language processing.\"]  \n",
    "hypothesis = \"NLTK is a great tool for NLP tasks.\"  \n",
    "bleu_score_value = bleu_score.sentence_bleu([reference], hypothesis)  \n",
    "meteor_score_value = single_meteor_score(reference[0], hypothesis)  \n",
    "\n",
    "#### Optional: Dependency Parsing\n",
    "parser = CoreNLPParser(url='http://localhost:9000')  \n",
    "parsed_tree = next(parser.raw_parse(text))  \n",
    "\n",
    "#### Optional: Chunking and Chinking\n",
    "grammar = r\"\"\"  \n",
    "  NP: {<DT>?<JJ>*<NN>}   # Chunk sequences of DT, JJ, NN  \n",
    "  NP: {<DT>?<NNP>+}      # Chunk sequences of DT, NNP  \n",
    "  \"\"\"  \n",
    "chunk_parser = nltk.RegexpParser(grammar)  \n",
    "chunked_tree = chunk_parser.parse(pos_tags)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899d947-9625-492f-a9c4-62f994f1af91",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4627c-3e09-41e9-86c9-11f6234760cc",
   "metadata": {},
   "source": [
    "import spacy  \n",
    "from spacy import displacy  \n",
    "from spacy.matcher import Matcher  \n",
    "from spacy.tokens import Doc  \n",
    "from spacy.lang.en.stop_words import STOP_WORDS  \n",
    "from collections import Counter  \n",
    "\n",
    "#### 1. Loading spaCy's English NLP Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "#### 2. Tokenization and Part-of-Speech Tagging\n",
    "text = \"spaCy is a popular library for natural language processing.\"  \n",
    "doc = nlp(text)  \n",
    "for token in doc:  \n",
    "    print(token.text, token.pos_)  \n",
    "\n",
    "#### 3. Named Entity Recognition\n",
    "for ent in doc.ents:  \n",
    "    print(ent.text, ent.label_)  \n",
    "\n",
    "#### 4. Dependency Parsing\n",
    "for token in doc:  \n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_)  \n",
    "\n",
    "#### 5. Visualization with displaCy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})  \n",
    "\n",
    "#### 6. Stopword Removal\n",
    "filtered_tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]  \n",
    "\n",
    "#### 7. Lemmatization\n",
    "lemmatized_words = [token.lemma_ for token in doc]  \n",
    "\n",
    "#### 8. Sentence Segmentation\n",
    "sentences = [sent.text for sent in doc.sents]  \n",
    "\n",
    "#### 9. Matching with spaCy's Matcher\n",
    "matcher = Matcher(nlp.vocab)  \n",
    "pattern = [{\"LOWER\": \"popular\"}, {\"LOWER\": \"library\"}]  \n",
    "matcher.add(\"PopularLibrary\", [pattern])  \n",
    "matches = matcher(doc)  \n",
    "\n",
    "#### 10. Frequency Analysis with Counter\n",
    "word_counter = Counter([token.text.lower() for token in doc])  \n",
    "\n",
    "#### Optional: Custom Rule-Based Entity Recognition\n",
    "def add_custom_entities(doc):  \n",
    "    doc.ents = [ent for ent in doc.ents if not ent.text.startswith('@')]  \n",
    "    return doc  \n",
    "\n",
    "Doc.set_extension(\"custom_entities\", getter=add_custom_entities)  \n",
    "doc_with_custom_ents = nlp(\"I'm talking about @spacy!\")  \n",
    "\n",
    "print(doc_with_custom_ents._.custom_entities)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6953b4-8b42-4987-8eeb-1ec3096fad3e",
   "metadata": {},
   "source": [
    "## gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691950c-a254-4241-ac3e-018c4f311df3",
   "metadata": {},
   "source": [
    "import gensim  \n",
    "from gensim.models import Word2Vec, KeyedVectors  \n",
    "from gensim.corpora import Dictionary  \n",
    "from gensim.utils import simple_preprocess  \n",
    "from gensim.models import TfidfModel  \n",
    "from gensim.similarities import MatrixSimilarity  \n",
    "from gensim.summarization import summarize  \n",
    "from gensim.models import LsiModel  \n",
    "from gensim.models import CoherenceModel  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  \n",
    "from gensim.models.phrases import Phrases, Phraser  \n",
    "from gensim.models.word2vec import LineSentence  \n",
    "\n",
    "#### 1. Word Embeddings with Word2Vec\n",
    "sentences = [[\"machine\", \"learning\", \"is\", \"fun\"], [\"natural\", \"language\", \"processing\", \"is\", \"challenging\"]]  \n",
    "model_w2v = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=0)  \n",
    "\n",
    "#### 2. Loading Pre-trained Word Vectors\n",
    "pretrained_model = KeyedVectors.load_word2vec_format('path/to/pretrained/model.bin', binary=True)  \n",
    "\n",
    "#### 3. Document Tokenization\n",
    "documents = [\"Gensim is a popular library for text analysis.\", \"It includes various NLP algorithms.\"]  \n",
    "tokenized_docs = [simple_preprocess(doc) for doc in documents]  \n",
    "\n",
    "#### 4. Creating a Dictionary and Bag-of-Words\n",
    "dictionary = Dictionary(tokenized_docs)  \n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]  \n",
    "\n",
    "#### 5. TF-IDF Model\n",
    "tfidf = TfidfModel(corpus)  \n",
    "tfidf_weights = tfidf[corpus]  \n",
    "\n",
    "#### 6. Similarity Matrix\n",
    "similarity_matrix = MatrixSimilarity(tfidf_weights)  \n",
    "\n",
    "#### 7. Text Summarization\n",
    "text = \"Gensim is a Python library for topic modeling and document similarity analysis.\"  \n",
    "summary = summarize(text)  \n",
    "\n",
    "#### 8. Latent Semantic Indexing (LSI)\n",
    "lsi_model = LsiModel(tfidf_weights, id2word=dictionary, num_topics=2)  \n",
    "lsi_topics = lsi_model.show_topics()  \n",
    "\n",
    "#### 9. Coherence Model\n",
    "coherence_model = CoherenceModel(model=lsi_model, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')  \n",
    "\n",
    "#### 10. Document Embeddings with Doc2Vec\n",
    "tagged_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]  \n",
    "model_doc2vec = Doc2Vec(tagged_documents, vector_size=50, window=2, min_count=1, epochs=10)  \n",
    "\n",
    "#### Optional: Phrase Detection\n",
    "phrases = Phrases(tokenized_docs, min_count=1, threshold=1)  \n",
    "bigram = Phraser(phrases)  \n",
    "tokenized_with_phrases = [bigram[doc] for doc in tokenized_docs]  \n",
    "\n",
    "#### Optional: Training Word2Vec on Large Corpus\n",
    "sentences = LineSentence('path/to/large/corpus.txt')  \n",
    "model_large_corpus = Word2Vec(sentences, vector_size=100, window=5, min_count=10, sg=0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7dd61-e790-4b0c-94e6-a7a81a92e4e9",
   "metadata": {},
   "source": [
    "## Transformers (Hugging Face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d149f6-80f5-44c5-9775-972b868125b8",
   "metadata": {},
   "source": [
    "import torch  \n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig  \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForMaskedLM  \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration  \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer  \n",
    "\n",
    "#### 1. Text Generation with GPT-2\n",
    "generator = GPT2LMHeadModel.from_pretrained(\"gpt2\")  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  \n",
    "input_text = \"Once upon a time\"  \n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")  \n",
    "output = generator.generate(input_ids, max_length=50, num_return_sequences=5)  \n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "\n",
    "#### 2. Question Answering with BERT\n",
    "question_answering = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", tokenizer=\"distilbert-base-cased\")  \n",
    "context = \"Hugging Face is a technology company based in New York.\"  \n",
    "question = \"Where is Hugging Face based?\"  \n",
    "result = question_answering(question=question, context=context)  \n",
    "answer = result['answer']  \n",
    "\n",
    "#### 3. Sentiment Analysis with BERT\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", tokenizer=\"nlptown/bert-base-  multilingual-uncased-sentiment\")  \n",
    "text = \"I love using the Transformers library!\"  \n",
    "sentiment = sentiment_classifier(text)[0]['label']  \n",
    "\n",
    "#### 4. Named Entity Recognition with BERT\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", tokenizer=\"dbmdz/bert-large-cased-finetuned-conll03-english\")  \n",
    "text = \"Apple Inc. is a technology company.\"  \n",
    "entities = ner_pipeline(text)  \n",
    "\n",
    "#### 5. Token Classification with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")  \n",
    "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")  \n",
    "text = \"Hugging Face is a great NLP library.\"  \n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  \n",
    "outputs = model(**inputs)  \n",
    "predicted_labels = torch.argmax(outputs.logits, dim=2)  \n",
    "predicted_entities = [tokenizer.decode(token_ids) for token_ids in predicted_labels[0]]  \n",
    "\n",
    "#### 6. Masked Language Modeling with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")  \n",
    "text = \"I want to [MASK] the world.\"  \n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  \n",
    "outputs = model(**inputs)\n",
    "masked_token_logits = outputs.logits[0, inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)]  \n",
    "\n",
    "#### 7. Sequence Classification with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")  \n",
    "text = \"Hugging Face is a great NLP library.\"  \n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  \n",
    "outputs = model(**inputs)  \n",
    "predicted_class = torch.argmax(outputs.logits)  \n",
    "\n",
    "#### 8. Summarization with T5\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")  \n",
    "article = \"Hugging Face is a company specializing in Natural Language Processing technologies.\"  \n",
    "summary = summarizer(article, max_length=50, min_length=10)[0]['summary_text']  \n",
    "\n",
    "#### 9. Translation with MarianMT\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")  \n",
    "text = \"Transformers is a versatile library for natural language processing.\"  \n",
    "translated_text = translator(text)[0]['translation_text']  \n",
    "\n",
    "#### 10. Custom Model and Tokenizer Configuration\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")  \n",
    "model = AutoModelForSequenceClassification.from_config(config)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766798a9-5eab-41bb-8b8f-6a8cd995c33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
