{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8718f02-26e3-4e43-8319-738050b27754",
   "metadata": {},
   "source": [
    "# Machine Learning and Data Science:\n",
    "\n",
    "###### **scikit-learn**: Machine learning library with various algorithms and tools.\n",
    "###### **TensorFlow**: Open-source machine learning framework developed by Google.\n",
    "###### **Keras**: High-level neural networks API (now part of TensorFlow).\n",
    "###### **PyTorch**: Deep learning framework with dynamic computation graphs.\n",
    "###### **XGBoost**: Gradient boosting library for supervised learning.\n",
    "###### **LightGBM**: Gradient boosting framework that uses histogram-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eafa62-8557-45dd-8f92-fc34ff138524",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8222be-c13f-44e8-ac9f-39c61a6cb6be",
   "metadata": {},
   "source": [
    "import numpy as np  \n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "\n",
    "#### 1. Loading a Dataset\n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "#### 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "#### 3. Linear Regression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)  \n",
    "predictions = regressor.predict(X_test)  \n",
    "\n",
    "#### 4. Clustering (K-Means)\n",
    "kmeans = KMeans(n_clusters=3)  \n",
    "kmeans.fit(X)  \n",
    "cluster_labels = kmeans.labels_  \n",
    "\n",
    "#### 5. Support Vector Machine (SVM)\n",
    "svm_classifier = SVC(kernel='linear')  \n",
    "svm_classifier.fit(X_train, y_train)  \n",
    "svm_predictions = svm_classifier.predict(X_test)  \n",
    "\n",
    "#### 6. Model Evaluation\n",
    "accuracy = accuracy_score(y_test, svm_predictions)  \n",
    "mse = mean_squared_error(y_test, predictions)  \n",
    "\n",
    "#### 7. Feature Scaling\n",
    "scaler = StandardScaler()  \n",
    "X_scaled = scaler.fit_transform(X)  \n",
    "\n",
    "#### 8. Text Vectorization (CountVectorizer)\n",
    "corpus = [\"This is a document.\", \"Another document.\", \"Yet another document.\"]  \n",
    "vectorizer = CountVectorizer()  \n",
    "X_vectorized = vectorizer.fit_transform(corpus)  \n",
    "\n",
    "#### 9. Dimensionality Reduction (PCA)\n",
    "pca = PCA(n_components=2)  \n",
    "X_reduced = pca.fit_transform(X_scaled)  \n",
    "\n",
    "#### 10. Creating a Pipeline\n",
    "pipeline = Pipeline([  \n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('pca', PCA(n_components=2)),  \n",
    "    ('classifier', SVC(kernel='linear'))  \n",
    "])  \n",
    "pipeline.fit(X_train, y_train)  \n",
    "pipeline_accuracy = pipeline.score(X_test, y_test)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cb8e2-46aa-46e7-a291-ac8fc3ab9f32",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce0ea2-994f-4855-baf3-55aacddf619b",
   "metadata": {},
   "source": [
    "import numpy as np  \n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten  \n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  \n",
    "from tensorflow.keras.applications import MobileNetV2  \n",
    "from tensorflow.keras.datasets import mnist  \n",
    "\n",
    "##### 1. Creating Tensors\n",
    "tensor = tf.constant([[1, 2, 3], [4, 5, 6]])  \n",
    "\n",
    "##### 2. Building a Sequential Model\n",
    "model = Sequential([  \n",
    "    Dense(64, activation='relu', input_shape=(784,)),  \n",
    "    Dense(10, activation='softmax')  \n",
    "])  \n",
    "\n",
    "##### 3. Loading a Dataset (MNIST)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  \n",
    "\n",
    "##### 4. Preprocessing Data\n",
    "X_train = X_train.reshape((-1, 784)).astype('float32') / 255.0  \n",
    "X_test = X_test.reshape((-1, 784)).astype('float32') / 255.0  \n",
    "\n",
    "##### 5. Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)  \n",
    "\n",
    "##### 6. Compiling and Training a Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  \n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))  \n",
    "\n",
    "##### 7. Model Evaluation\n",
    "y_pred = model.predict(X_test)  \n",
    "accuracy = accuracy_score(y_test, np.argmax(y_pred, axis=1))  \n",
    "\n",
    "##### 8. Data Augmentation (Image Data Generator)\n",
    "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)  \n",
    "datagen.fit(X_train)  \n",
    "\n",
    "##### 9. Transfer Learning\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))  \n",
    "model = Sequential([  \n",
    "    base_model,  \n",
    "    GlobalAveragePooling2D(),  \n",
    "    Dense(10, activation='softmax')  \n",
    "])  \n",
    "\n",
    "##### 10. Model Saving and Loading\n",
    "model.save('my_model.h5')  \n",
    "loaded_model = keras.models.load_model('my_model.h5')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44c9c4-2de4-4f26-a4f7-2d6b77d78b1c",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e7d10-7d6f-4ff4-9709-cb45174ccc5c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#### 1. Creating a Sequential Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "#### 2. Loading a Dataset (MNIST)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  \n",
    "\n",
    "#### 3. Preprocessing Data\n",
    "X_train = X_train.reshape((-1, 784)).astype('float32') / 255.0  \n",
    "X_test = X_test.reshape((-1, 784)).astype('float32') / 255.0  \n",
    "\n",
    "#### 4. Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)  \n",
    "\n",
    "#### 5. Compiling and Training a Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  \n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))  \n",
    "\n",
    "#### 6. Model Evaluation\n",
    "y_pred = model.predict(X_test)  \n",
    "accuracy = accuracy_score(y_test, np.argmax(y_pred, axis=1))  \n",
    "\n",
    "#### 7. Data Augmentation (Image Data Generator)\n",
    "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)  \n",
    "datagen.fit(X_train)  \n",
    "\n",
    "#### 8. Transfer Learning\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))  \n",
    "model = Sequential([  \n",
    "    base_model,  \n",
    "    GlobalAveragePooling2D(),  \n",
    "    Dense(10, activation='softmax')  \n",
    "])  \n",
    "\n",
    "#### 9. Model Saving and Loading\n",
    "model.save('my_model.h5')  \n",
    "loaded_model = load_model('my_model.h5')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14fe92-5f6b-4020-973b-4d406dc6f3b4",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcfada-a7cb-4bcb-94d6-51a0f31f14a3",
   "metadata": {},
   "source": [
    "import numpy as np  \n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torchvision import datasets, transforms  \n",
    "from torch.utils.data import DataLoader, random_split  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "import torchvision.models as models  \n",
    "\n",
    "#### 1. Creating Tensors\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])  \n",
    "\n",
    "#### 2. Defining a Simple Neural Network\n",
    "class SimpleNN(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SimpleNN, self).__init__()  \n",
    "        self.fc1 = nn.Linear(784, 64)  \n",
    "        self.fc2 = nn.Linear(64, 10)  \n",
    "    \n",
    "    def forward(self, x):  \n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = torch.softmax(self.fc2(x), dim=-1)  \n",
    "        return x  \n",
    "\n",
    "#### 3. Loading a Dataset (MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])  \n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)  \n",
    "\n",
    "#### 4. Train-Validation Split\n",
    "train_size = int(0.9 * len(mnist_dataset))  \n",
    "val_size = len(mnist_dataset) - train_size  \n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])  \n",
    "\n",
    "#### 5. Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)  \n",
    "\n",
    "#### 6. Model Initialization and Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model = SimpleNN().to(device)  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "#### Training loop\n",
    "for epoch in range(5):  \n",
    "    model.train()  \n",
    "    for inputs, labels in train_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs.view(-1, 784))  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "#### 7. Model Evaluation\n",
    "model.eval()  \n",
    "y_true, y_pred = [], []  \n",
    "with torch.no_grad():  \n",
    "    for inputs, labels in val_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        outputs = model(inputs.view(-1, 784))  \n",
    "        predicted_labels = torch.argmax(outputs, dim=1)  \n",
    "        y_true.extend(labels.cpu().numpy())  \n",
    "        y_pred.extend(predicted_labels.cpu().numpy())  \n",
    "accuracy = accuracy_score(y_true, y_pred)  \n",
    "\n",
    "#### 8. Transfer Learning\n",
    "pretrained_model = models.resnet18(pretrained=True)  \n",
    "num_features = pretrained_model.fc.in_features  \n",
    "pretrained_model.fc = nn.Linear(num_features, 10)  \n",
    "\n",
    "#### 9. Saving and Loading Model\n",
    "torch.save(model.state_dict(), 'my_model.pth')  \n",
    "loaded_model = SimpleNN()  \n",
    "loaded_model.load_state_dict(torch.load('my_model.pth'))  \n",
    "loaded_model.eval()  \n",
    "\n",
    "#### 10. GPU Acceleration\n",
    "if torch.cuda.is_available():  \n",
    "    model = model.to('cuda')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0b37d-8d68-4d69-8dd1-bca3f2605100",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b091eb-4ead-424c-a318-c0f733927bb8",
   "metadata": {},
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import xgboost as xgb  \n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  \n",
    "from xgboost import XGBClassifier, XGBRegressor  \n",
    "from xgboost import plot_importance  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "#### 1. Loading a Dataset (Iris)\n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "#### 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "#### 3. Classification with XGBoost\n",
    "classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)  \n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "accuracy = accuracy_score(y_test, y_pred)  \n",
    "\n",
    "#### 4. Regression with XGBoost\n",
    "regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)  \n",
    "regressor.fit(X_train, y_train)  \n",
    "y_pred = regressor.predict(X_test)  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "\n",
    "#### 5. Plot Feature Importance\n",
    "plot_importance(regressor)  \n",
    "plt.show()  \n",
    "\n",
    "#### 6. Custom Objective Function (Example)\n",
    "def custom_obj(y_true, y_pred):  \n",
    "    grad = y_pred - y_true  \n",
    "    hess = np.ones_like(y_pred)  \n",
    "    return grad, hess  \n",
    "\n",
    "#### 7. Early Stopping\n",
    "eval_set = [(X_test, y_test)]\n",
    "classifier.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set)  \n",
    "\n",
    "#### 8. Cross-Validation\n",
    "cv_results = xgb.cv(dtrain=xgb.DMatrix(X_train, label=y_train), params=classifier.get_params(), nfold=5,  \n",
    "                    num_boost_round=100, early_stopping_rounds=10, metrics=\"logloss\", seed=42)  \n",
    "\n",
    "#### 9. Saving and Loading Model\n",
    "classifier.save_model('xgboost_model.json')  \n",
    "loaded_model = xgb.Booster(model_file='xgboost_model.json')  \n",
    "\n",
    "#### 10. Hyperparameter Tuning (Example)\n",
    "param_grid = {  \n",
    "    'learning_rate': [0.1, 0.01],  \n",
    "    'max_depth': [3, 5, 7],  \n",
    "    'n_estimators': [50, 100, 200]  \n",
    "}  \n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', cv=3)  \n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15e7ab-8268-4267-90ba-0e3c65e8037a",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3a8b2-9112-42fd-84af-a1b43e76f809",
   "metadata": {},
   "source": [
    "import numpy as np  \n",
    "import lightgbm as lgb  \n",
    "from sklearn.datasets import load_iris  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  \n",
    "from lightgbm import LGBMClassifier, LGBMRegressor  \n",
    "from lightgbm import plot_importance  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "\n",
    "#### 1. Loading a Dataset (Iris)\n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "#### 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "#### 3. Classification with LightGBM\n",
    "classifier = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)  \n",
    "classifier.fit(X_train, y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "accuracy = accuracy_score(y_test, y_pred)  \n",
    "\n",
    "#### 4. Regression with LightGBM\n",
    "regressor = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)  \n",
    "regressor.fit(X_train, y_train)  \n",
    "y_pred = regressor.predict(X_test)  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "\n",
    "#### 5. Plot Feature Importance\n",
    "plot_importance(regressor)  \n",
    "plt.show()  \n",
    "\n",
    "#### 6. Custom Objective Function (Example)\n",
    "def custom_obj(y_true, y_pred, grad, hess):  \n",
    "    grad[:] = y_pred - y_true  \n",
    "    hess[:] = np.ones_like(y_pred)  \n",
    "    return grad, hess  \n",
    "\n",
    "#### 7. Early Stopping\n",
    "eval_set = [(X_test, y_test)]\n",
    "classifier.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set, eval_metric=\"logloss\")  \n",
    "\n",
    "#### 8. Cross-Validation\n",
    "cv_results = lgb.cv(params=classifier.get_params(), train_set=lgb.Dataset(X_train, label=y_train),  \n",
    "                    num_boost_round=100, early_stopping_rounds=10, metrics=\"logloss\", stratified=False)  \n",
    "\n",
    "#### 9. Saving and Loading Model\n",
    "classifier.booster_.save_model('lightgbm_model.txt')  \n",
    "loaded_model = lgb.Booster(model_file='lightgbm_model.txt')  \n",
    "\n",
    "#### 10. Hyperparameter Tuning (Example)\n",
    "param_grid = {  \n",
    "    'learning_rate': [0.1, 0.01],  \n",
    "    'max_depth': [3, 5, 7],  \n",
    "    'n_estimators': [50, 100, 200]  \n",
    "}  \n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', cv=3)  \n",
    "grid_search.fit(X_train, y_train)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd79f8-ceb2-4975-afc1-e5ef62df8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
